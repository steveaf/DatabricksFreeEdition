{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af53f140-616d-4f1b-8167-6e49c1b12d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "rel_path = \"airport_full_20251201.txt\"\n",
    "match = re.search(r'20\\d{2}(0[1-9]|1[0-2])(0[1-9]|[12][0-9]|3[01])', rel_path)\n",
    "yyyymmdd = match.group(0) if match else None\n",
    "\n",
    "try:\n",
    "    file_date = datetime.strptime(yyyymmdd, \"%Y%m%d\").date() if yyyymmdd else None\n",
    "except Exception:\n",
    "    file_date = None\n",
    "display(file_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd4c38f-b527-44dc-b6a4-0ae1624d2999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "rel_path1 = \"airport_full_20251201.txt\"\n",
    "rel_path2 = \"airport_full_20251202.txt\"\n",
    "rel_path3 = \"airport_full_20251203.txt\"\n",
    "\n",
    "df1 = spark.read.option(\"header\", \"true\").option(\"sep\", \",\").csv(f\"/Volumes/demos_standard/public_data/public_raw/Airport_Batches/{rel_path1}\")\n",
    "df2 = spark.read.option(\"header\", \"true\").option(\"sep\", \",\").csv(f\"/Volumes/demos_standard/public_data/public_raw/Airport_Batches/{rel_path2}\")\n",
    "df3 = spark.read.option(\"header\", \"true\").option(\"sep\", \",\").csv(f\"/Volumes/demos_standard/public_data/public_raw/Airport_Batches/{rel_path3}\")\n",
    "\n",
    "df2_ins = df2.join(\n",
    "    df1,\n",
    "    on=\"ident\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "df2_del = df1.join(\n",
    "    df2,\n",
    "    on=\"ident\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "df3_ins = df3.join(\n",
    "    df2,\n",
    "    on=\"ident\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "df3_del = df2.join(\n",
    "    df3,\n",
    "    on=\"ident\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "# Find changed rows: same ident, any field difference\n",
    "join_cols = [c for c in df1.columns if c == \"ident\"]\n",
    "other_cols = [c for c in df1.columns if c != \"ident\"]\n",
    "\n",
    "df2_chg = df2.alias(\"d2\").join(\n",
    "    df1.alias(\"d1\"),\n",
    "    on=\"ident\",\n",
    "    how=\"inner\"\n",
    ").where(\n",
    "    \" OR \".join([f\"d2.{c} != d1.{c} OR (d2.{c} IS NULL AND d1.{c} IS NOT NULL) OR (d2.{c} IS NOT NULL AND d1.{c} IS NULL)\" for c in other_cols])\n",
    ").select(\"d2.*\")\n",
    "\n",
    "df3_chg = df3.alias(\"d3\").join(\n",
    "    df2.alias(\"d2\"),\n",
    "    on=\"ident\",\n",
    "    how=\"inner\"\n",
    ").where(\n",
    "    \" OR \".join([f\"d3.{c} != d2.{c} OR (d3.{c} IS NULL AND d2.{c} IS NOT NULL) OR (d3.{c} IS NOT NULL AND d2.{c} IS NULL)\" for c in other_cols])\n",
    ").select(\"d3.*\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"df1: {df1.count()}\")\n",
    "print(f\"df2: {df2.count()}\")\n",
    "print(f\"df3: {df3.count()}\")\n",
    "\n",
    "print(f\"df2_ins : {df2_ins.count()}\")\n",
    "print(f\"df2_del : {df2_del.count()}\")\n",
    "print(f\"df2_chg : {df2_chg.count()}\")\n",
    "\n",
    "# display(df2_ins)\n",
    "# display(df2_del)\n",
    "# display(df2_chg)\n",
    "\n",
    "print(f\"df3_ins : {df3_ins.count()}\")\n",
    "print(f\"d3_del : {df3_del.count()}\")\n",
    "print(f\"df3_chg : {df3_chg.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee68fe1-246c-4952-9eb6-fe7b551b8a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d98b35-aff0-477a-8fd4-b398032087d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import py7zr\n",
    "\n",
    "full_path_7z = \"/Volumes/demos_standard/public_data/public_raw/airport-codes.7z\"\n",
    "extract_dir = \"/Volumes/demos_standard/public_data/public_raw/tmp/airport_codes_extract\"\n",
    "\n",
    "with py7zr.SevenZipFile(full_path_7z, mode='r') as archive:\n",
    "    archive.extractall(path=extract_dir)\n",
    "\n",
    "import os\n",
    "csv_file = [f for f in os.listdir(extract_dir) if f.endswith('.csv')][0]\n",
    "csv_path = os.path.join(extract_dir, csv_file)\n",
    "\n",
    "df_all = spark.read.option(\"header\", \"true\").option(\"sep\", \",\").csv(csv_path)\n",
    "print(df_all.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ae40c4-87f4-45b1-a5d5-ed1b0cabb58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total records: {df_all.count()}\")\n",
    "print(f\"Distinct ident count: {df_all.select('ident').distinct().count()}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "manual_verify",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
