{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9acdb675-d9eb-41be-b184-842e0bd58071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_date_from_string(input_string):\n",
    "    match = re.search(\n",
    "      # r'20\\d{2}[-_]?(0[1-9]|1[0-2])[-_]?(0[1-9]|[12]\\d|3[01])',\n",
    "        r'20\\d\\d[-_]?(?:0[1-9]|1[0-2])[-_]?(?:0[1-9]|[12]\\d|3[01])',\n",
    "        input_string\n",
    "    )\n",
    "    if not match:\n",
    "        return None\n",
    "    try:\n",
    "        date_str = match.group(0).replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "        return datetime.strptime(date_str, \"%Y%m%d\").date()\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e0964e-7284-4530-aaac-4c681f3b5a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"date from name: {extract_date_from_string('airport_full_20251231.txt')}\")\n",
    "print(f\"date from 20251201: {extract_date_from_string('20251201')}\")\n",
    "print(f\"date from some-file-2025-12-01morestuff.csv': {extract_date_from_string('some-file-2025-12-01morestuff.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af53f140-616d-4f1b-8167-6e49c1b12d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "rel_path = \"airport_full_20251201.txt\"\n",
    "match = re.search(r'20\\d{2}(0[1-9]|1[0-2])(0[1-9]|[12][0-9]|3[01])', rel_path)\n",
    "yyyymmdd = match.group(0) if match else None\n",
    "\n",
    "try:\n",
    "    file_date = datetime.strptime(yyyymmdd, \"%Y%m%d\").date() if yyyymmdd else None\n",
    "except Exception:\n",
    "    file_date = None\n",
    "display(file_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99bed3fa-b348-4a22-abd8-a619f358a1f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def airports_raw():\n",
    "#   return (\n",
    "#     spark.read\n",
    "#       .option(\"header\", \"true\")\n",
    "#       .option(\"sep\", \",\")\n",
    "#       .csv(file_path)\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd4c38f-b527-44dc-b6a4-0ae1624d2999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "rel_path1 = \"airport_full_20251201.txt\"\n",
    "rel_path2 = \"airport_full_20251202.txt\"\n",
    "rel_path3 = \"airport_full_20251203.txt\"\n",
    "\n",
    "df1 = spark.read.option(\"header\", \"true\").option(\"sep\", \",\").csv(f\"/Volumes/demos_standard/public_data/public_raw/Airport_Batches/{rel_path1}\")\n",
    "df2 = spark.read.option(\"header\", \"true\").option(\"sep\", \",\").csv(f\"/Volumes/demos_standard/public_data/public_raw/Airport_Batches/{rel_path2}\")\n",
    "df3 = spark.read.option(\"header\", \"true\").option(\"sep\", \",\").csv(f\"/Volumes/demos_standard/public_data/public_raw/Airport_Batches/{rel_path3}\")\n",
    "\n",
    "df2_ins = df2.join(\n",
    "    df1,\n",
    "    on=\"ident\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "df2_del = df1.join(\n",
    "    df2,\n",
    "    on=\"ident\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "df3_ins = df3.join(\n",
    "    df2,\n",
    "    on=\"ident\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "df3_del = df2.join(\n",
    "    df3,\n",
    "    on=\"ident\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "# Find changed rows: same ident, any field difference\n",
    "join_cols = [c for c in df1.columns if c == \"ident\"]\n",
    "other_cols = [c for c in df1.columns if c != \"ident\"]\n",
    "\n",
    "df2_chg = df2.alias(\"d2\").join(\n",
    "    df1.alias(\"d1\"),\n",
    "    on=\"ident\",\n",
    "    how=\"inner\"\n",
    ").where(\n",
    "    \" OR \".join([f\"d2.{c} != d1.{c} OR (d2.{c} IS NULL AND d1.{c} IS NOT NULL) OR (d2.{c} IS NOT NULL AND d1.{c} IS NULL)\" for c in other_cols])\n",
    ").select(\"d2.*\")\n",
    "\n",
    "df3_chg = df3.alias(\"d3\").join(\n",
    "    df2.alias(\"d2\"),\n",
    "    on=\"ident\",\n",
    "    how=\"inner\"\n",
    ").where(\n",
    "    \" OR \".join([f\"d3.{c} != d2.{c} OR (d3.{c} IS NULL AND d2.{c} IS NOT NULL) OR (d3.{c} IS NOT NULL AND d2.{c} IS NULL)\" for c in other_cols])\n",
    ").select(\"d3.*\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"df1: {df1.count()}\")\n",
    "print(f\"df2: {df2.count()}\")\n",
    "print(f\"df3: {df3.count()}\")\n",
    "\n",
    "print(f\"df2_ins : {df2_ins.count()}\")\n",
    "print(f\"df2_del : {df2_del.count()}\")\n",
    "print(f\"df2_chg : {df2_chg.count()}\")\n",
    "\n",
    "# display(df2_ins)\n",
    "# display(df2_del)\n",
    "# display(df2_chg)\n",
    "\n",
    "print(f\"df3_ins : {df3_ins.count()}\")\n",
    "print(f\"d3_del : {df3_del.count()}\")\n",
    "print(f\"df3_chg : {df3_chg.count()}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "manual_verify",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
